{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import socket\n",
    "import json\n",
    "import cv2\n",
    "\n",
    "import logging as log\n",
    "import paho.mqtt.client as mqtt\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "from inference import Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def build_argparser():\n",
    "    \"\"\"\n",
    "    Parse command line arguments.\n",
    "\n",
    "    :return: command line arguments\n",
    "    \"\"\"\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument(\"-m\", \"--model\", required=True, type=str,\n",
    "                        help=\"Path to an xml file with a trained model.\")\n",
    "    parser.add_argument(\"-i\", \"--input\", required=True, type=str,\n",
    "                        help=\"Path to image or video file\")\n",
    "    parser.add_argument(\"-l\", \"--cpu_extension\", required=False, type=str,\n",
    "                        default=None,\n",
    "                        help=\"MKLDNN (CPU)-targeted custom layers.\"\n",
    "                             \"Absolute path to a shared library with the\"\n",
    "                             \"kernels impl.\")\n",
    "    parser.add_argument(\"-d\", \"--device\", type=str, default=\"CPU\",\n",
    "                        help=\"Specify the target device to infer on: \"\n",
    "                             \"CPU, GPU, FPGA or MYRIAD is acceptable. Sample \"\n",
    "                             \"will look for a suitable plugin for device \"\n",
    "                             \"specified (CPU by default)\")\n",
    "    parser.add_argument(\"-pt\", \"--prob_threshold\", type=float, default=0.5,\n",
    "                        help=\"Probability threshold for detections filtering\"\n",
    "                        \"(0.5 by default)\")\n",
    "    parser.add_argument(\"-pc\", \"--perf_counts\", type=str, default=False,\n",
    "                        help=\"Print performance counters\")\n",
    "    return parser\n",
    "\n",
    "\n",
    "def performance_counts(perf_count):\n",
    "    \"\"\"\n",
    "    print information about layers of the model.\n",
    "\n",
    "    :param perf_count: Dictionary consists of status of the layers.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    print(\"{:<70} {:<15} {:<15} {:<15} {:<10}\".format('name', 'layer_type',\n",
    "                                                      'exec_type', 'status',\n",
    "                                                      'real_time, us'))\n",
    "    for layer, stats in perf_count.items():\n",
    "        print(\"{:<70} {:<15} {:<15} {:<15} {:<10}\".format(layer,\n",
    "                                                          stats['layer_type'],\n",
    "                                                          stats['exec_type'],\n",
    "                                                          stats['status'],\n",
    "                                                          stats['real_time']))\n",
    "\n",
    "\n",
    "def ssd_out(frame, result):\n",
    "    \"\"\"\n",
    "    Parse SSD output.\n",
    "\n",
    "    :param frame: frame from camera/video\n",
    "    :param result: list contains the data to parse ssd\n",
    "    :return: person count and frame\n",
    "    \"\"\"\n",
    "    current_count = 0\n",
    "    for obj in result[0][0]:\n",
    "        # Draw bounding box for object when it's probability is more than\n",
    "        #  the specified threshold\n",
    "        if obj[2] > prob_threshold:\n",
    "            xmin = int(obj[3] * initial_w)\n",
    "            ymin = int(obj[4] * initial_h)\n",
    "            xmax = int(obj[5] * initial_w)\n",
    "            ymax = int(obj[6] * initial_h)\n",
    "            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 55, 255), 1)\n",
    "            current_count = current_count + 1\n",
    "    return frame, current_count\n",
    "\n",
    "\n",
    "def ssd_out(frame, result, pt, H, W):\n",
    "    \"\"\"\n",
    "    Parse SSD output.\n",
    "\n",
    "    :param frame: frame from camera/video\n",
    "    :param result: list contains the data to parse ssd\n",
    "    :return: person count and frame\n",
    "    \"\"\"\n",
    "    current_count = 0\n",
    "    boxes = []\n",
    "    confidences = []\n",
    "    classIDs = []\n",
    "\n",
    "    for obj in result[0][0]:\n",
    "        # Draw bounding box for object when it's probability is more than\n",
    "        #  the specified threshold\n",
    "        if obj[2] > prob_threshold:\n",
    "            xmin = int(obj[3] * initial_w)\n",
    "            ymin = int(obj[4] * initial_h)\n",
    "            xmax = int(obj[5] * initial_w)\n",
    "            ymax = int(obj[6] * initial_h)\n",
    "            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 55, 255), 1)\n",
    "            current_count = current_count + 1\n",
    "\n",
    "            boxes.append([xmin, ymin, int(xmax-xmin), int(ymax-ymin)])\n",
    "            confidences.append(float(obj[2]))\n",
    "            classIDs.append(obj[1])\n",
    "    \n",
    "    idxs = cv2.dnn.NMSBoxes(boxes, confidences, pt, pt)\n",
    "\n",
    "    if len(idxs) > 0:\n",
    "\n",
    "        status = list()\n",
    "        idf = idxs.flatten()\n",
    "        close_pair = list()\n",
    "        s_close_pair = list()\n",
    "        center = list()\n",
    "        dist = list()\n",
    "        for i in idf:\n",
    "            (x, y) = (boxes[i][0], boxes[i][1])\n",
    "            (w, h) = (boxes[i][2], boxes[i][3])\n",
    "            center.append([int(x + w / 2), int(y + h / 2)])\n",
    "\n",
    "            status.append(0)\n",
    "        for i in range(len(center)):\n",
    "            for j in range(len(center)):\n",
    "                g = isclose(center[i], center[j])\n",
    "\n",
    "                if g == 1:\n",
    "\n",
    "                    close_pair.append([center[i], center[j]])\n",
    "                    status[i] = 1\n",
    "                    status[j] = 1\n",
    "                elif g == 2:\n",
    "                    s_close_pair.append([center[i], center[j]])\n",
    "                    if status[i] != 1:\n",
    "                        status[i] = 2\n",
    "                    if status[j] != 1:\n",
    "                        status[j] = 2\n",
    "\n",
    "        total_p = len(center)\n",
    "        low_risk_p = status.count(2)\n",
    "        high_risk_p = status.count(1)\n",
    "        safe_p = status.count(0)\n",
    "        kk = 0\n",
    "\n",
    "        for i in idf:\n",
    "\n",
    "            tot_str = \"TOTAL COUNT: \" + str(total_p)\n",
    "            high_str = \"HIGH RISK COUNT: \" + str(high_risk_p)\n",
    "            low_str = \"LOW RISK COUNT: \" + str(low_risk_p)\n",
    "            safe_str = \"SAFE COUNT: \" + str(safe_p)\n",
    "\n",
    "            sub_img = frame[H - 120:H, 0:210]\n",
    "            black_rect = np.ones(sub_img.shape, dtype=np.uint8) * 0\n",
    "\n",
    "            res = cv2.addWeighted(sub_img, 0.8, black_rect, 0.2, 1.0)\n",
    "\n",
    "            frame[H - 120:H, 0:210] = res\n",
    "\n",
    "            cv2.putText(frame, tot_str, (10, H - 90),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)\n",
    "            cv2.putText(frame, safe_str, (10, H - 65),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 1)\n",
    "            cv2.putText(frame, low_str, (10, H - 40),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 120, 255), 1)\n",
    "            cv2.putText(frame, high_str, (10, H - 15),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 150), 1)\n",
    "\n",
    "            (x, y) = (boxes[i][0], boxes[i][1])\n",
    "            (w, h) = (boxes[i][2], boxes[i][3])\n",
    "            if status[kk] == 1:\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 0, 150), 2)\n",
    "\n",
    "            elif status[kk] == 0:\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "            else:\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 120, 255), 2)\n",
    "\n",
    "            kk += 1\n",
    "        for h in close_pair:\n",
    "            cv2.line(frame, tuple(h[0]), tuple(h[1]), (0, 0, 255), 2)\n",
    "        for b in s_close_pair:\n",
    "            cv2.line(frame, tuple(b[0]), tuple(b[1]), (0, 255, 255), 2)\n",
    "\n",
    "    return frame, current_count\n",
    "\n",
    "def calibrated_dist(p1, p2):\n",
    "    return ((p1[0] - p2[0]) ** 2 + 550 / ((p1[1] + p2[1]) / 2) * (p1[1] - p2[1]) ** 2) ** 0.5\n",
    "\n",
    "\n",
    "def isclose(p1, p2):\n",
    "    c_d = calibrated_dist(p1, p2)\n",
    "    calib = (p1[1] + p2[1]) / 2\n",
    "    if 0 < c_d < 0.15 * calib:\n",
    "        return 1\n",
    "    elif 0 < c_d < 0.2 * calib:\n",
    "        return 2\n",
    "    else:\n",
    "        return 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cur_request_id = 0\n",
    "last_count = 0\n",
    "total_count = 0\n",
    "start_time = 0\n",
    "\n",
    "MODEL = '/opt/intel/openvino/deployment_tools/tools/model_downloader/Retail/object_detection/pedestrian/rmnet_ssd/0013/dldt/person-detection-retail-0013.xml'\n",
    "#MODEL = '/opt/intel/openvino/deployment_tools/model_optimizer/yolov3-tiny_model_1920x1920.xml'\n",
    "#MODEL = '/opt/intel/openvino/deployment_tools/model_optimizer/yolov3_model_1920x1920.xml'\n",
    "CPU_EXT = '/opt/intel/openvino/deployment_tools/inference_engine/lib/intel64/libcpu_extension_sse4.so'\n",
    "PT = 0.6\n",
    "\n",
    "# Initialise the class\n",
    "infer_network = Network()\n",
    "# Load the network to IE plugin to get shape of input layer\n",
    "n, c, h, w = infer_network.load_model(MODEL, 'CPU', 1, 1,\n",
    "                                      cur_request_id, CPU_EXT)[1]\n",
    "\n",
    "\n",
    "# Checks for input image\n",
    "single_image_mode = True\n",
    "input_stream = './TownCenter_frame.jpg'\n",
    "\n",
    "cap = cv2.VideoCapture(input_stream)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    log.error(\"ERROR! Unable to open video source\")\n",
    "global initial_w, initial_h, prob_threshold\n",
    "prob_threshold = PT\n",
    "initial_w = cap.get(3)\n",
    "initial_h = cap.get(4)\n",
    "\n",
    "flag, frame = cap.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'H' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-867896d3601d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Results of the output layer of the network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfer_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_request_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mssd_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob_threshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mresol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCAP_PROP_FRAME_WIDTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'x'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCAP_PROP_FRAME_HEIGHT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-38-1c32e24e26b6>\u001b[0m in \u001b[0;36mssd_out\u001b[0;34m(frame, result, pt)\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0msafe_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"SAFE COUNT: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msafe_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m             \u001b[0msub_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mH\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m120\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m210\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m             \u001b[0mblack_rect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'H' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Start async inference\n",
    "image = cv2.resize(frame, (w, h))\n",
    "# Change data layout from HWC to CHW\n",
    "image = image.transpose((2, 0, 1))\n",
    "image = image.reshape((n, c, h, w))\n",
    "# Start asynchronous inference for specified request.\n",
    "inf_start = time.time()\n",
    "infer_network.exec_net(cur_request_id, image)\n",
    "# Wait for the result\n",
    "if infer_network.wait(cur_request_id) == 0:\n",
    "    det_time = time.time() - inf_start\n",
    "    # Results of the output layer of the network\n",
    "    result = infer_network.get_output(cur_request_id)\n",
    "    frame, current_count = ssd_out(frame, result, prob_threshold)\n",
    "\n",
    "    resol = str(int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))) + 'x' + str(int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
    "\n",
    "    last_count = current_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imshow('image',frame)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 1.        , 0.99832875, 0.12558687, 0.7252936 ,\n",
       "       0.18883118, 0.9964052 ], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0][0][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
